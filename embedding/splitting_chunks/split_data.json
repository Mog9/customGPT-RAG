{
  "attention.txt": [
    "The attention mechanism allows a model to focus on the most relevant parts of the input when making",
    "input when making predictions. Instead of treating each word equally, attention assigns different",
    "assigns different weights to tokens based on their importance in context. For example, in the",
    "For example, in the sentence \u201cThe cat sat on the mat because it was tired,\u201d the model uses",
    "the model uses attention to understand that \u201cit\u201d refers to \u201ccat,\u201d not \u201cmat.\u201d",
    "In transformers, this is implemented as self-attention, where each token looks at every other token",
    "every other token in the sequence to decide what to focus on. This gives the model a global view of",
    "a global view of context \u2014 unlike RNNs, which could only remember recent words. Multi-head",
    "words. Multi-head attention expands this further by letting the model learn multiple types of",
    "multiple types of relationships simultaneously (e.g., syntactic and semantic).",
    "Attention is often visualized as a heatmap showing how much one word \u201cattends\u201d to another. It is",
    "to another. It is the key innovation that made transformers so effective, allowing them to handle",
    "them to handle context, ambiguity, and long sentences with incredible accuracy. In short, attention",
    "In short, attention is what gives models \u201cunderstanding\u201d in language."
  ],
  "transformers.txt": [
    "Transformers are the core architecture behind modern large language models (LLMs). Introduced by",
    "Introduced by Vaswani et al. in 2017\u2019s \u201cAttention is All You Need\u201d, the transformer revolutionized",
    "revolutionized NLP by replacing recurrent networks (RNNs) with a structure that can process entire",
    "can process entire sequences in parallel. This allowed models to scale massively and understand",
    "and understand context over long passages of text.",
    "The transformer architecture is built around two main blocks: the encoder and the decoder. The",
    "the decoder. The encoder reads and represents the input text, while the decoder generates the",
    "generates the output (for example, a translation or a chatbot response). Models like BERT use only",
    "like BERT use only the encoder for understanding tasks, while GPT uses only the decoder for",
    "the decoder for generating text. This flexibility is part of what makes transformers so powerful.",
    "Transformers also rely heavily on embeddings and attention mechanisms to understand meaning and",
    "meaning and relationships between tokens. They enable models to handle long-range dependencies \u2014",
    "dependencies \u2014 like connecting the subject and verb in a long sentence \u2014 something earlier models",
    "earlier models struggled with. Almost every modern GenAI system today, from ChatGPT to Stable",
    "ChatGPT to Stable Diffusion (for text-to-image), is built on the transformer foundation."
  ],
  "tokenization.txt": [
    "Tokenization is the process of breaking text into smaller pieces, called tokens, that can be",
    "tokens, that can be understood by language models. Tokens can be words, characters, or subwords",
    "or subwords depending on the tokenizer used. For example, \u201cunbelievable\u201d might be split into \u201cun\u201d,",
    "be split into \u201cun\u201d, \u201cbeliev\u201d, and \u201cable\u201d \u2014 this helps the model learn word patterns and meanings",
    "and meanings even for unseen words. Tokenization makes raw text mathematically representable and",
    "representable and consistent across all inputs.",
    "In practice, tokenizers use rules or pretrained vocabularies. GPT and BERT models, for instance,",
    "for instance, use Byte Pair Encoding (BPE) or WordPiece, which merge frequently occurring sequences",
    "occurring sequences of characters. This way, the tokenizer balances vocabulary size and token",
    "size and token length efficiently. These methods also handle unknown words gracefully by breaking",
    "by breaking them into known sub-pieces.",
    "Good tokenization is crucial because it directly affects how well a model can understand context.",
    "understand context. If the tokenizer splits words poorly, the model may lose semantic meaning.",
    "semantic meaning. Conversely, efficient tokenization reduces computational load and allows models",
    "and allows models to generalize across languages, typos, and new terms."
  ],
  "embedding.txt": [
    "Embeddings are vector representations of text that capture meaning in numbers. Every token (like",
    "Every token (like \u201ccat\u201d or \u201crun\u201d) is converted into a high-dimensional vector that represents its",
    "that represents its semantic context. Words that are similar in meaning have embeddings that are",
    "embeddings that are close to each other in vector space \u2014 for instance, \u201cking\u201d and \u201cqueen\u201d or",
    "and \u201cqueen\u201d or \u201cwalk\u201d and \u201crun.\u201d These embeddings become the foundation of how AI models understand",
    "models understand and compare words.",
    "Early models like Word2Vec or GloVe built embeddings by looking at how words appear in context",
    "appear in context within large text corpora. Modern transformer-based models like BERT or GPT learn",
    "BERT or GPT learn contextual embeddings, meaning the vector for a word changes depending on its",
    "depending on its surrounding words \u2014 \u201cbank\u201d in \u201criver bank\u201d and \u201cmoney bank\u201d will have different",
    "will have different vectors.",
    "Embeddings are also key in Retrieval-Augmented Generation (RAG), where your documents are converted",
    "are converted into embeddings and stored in a vector database. When a user asks a question, the",
    "a question, the system retrieves the most relevant pieces of text based on embedding similarity,",
    "similarity, enabling the model to answer accurately using external knowledge."
  ],
  "pluto.txt": [
    "Pluto is a Streamlit-based no-code machine learning tool that allows users to train models directly",
    "models directly in the browser without writing any code. It enables users to upload any CSV",
    "to upload any CSV dataset, automatically detect the type of machine learning task\u2014either",
    "task\u2014either classification or regression\u2014visualize key data insights, and train models instantly.",
    "models instantly. The project is designed to simplify the entire machine learning workflow, making",
    "workflow, making it accessible to users without any programming experience.",
    "When a user uploads a CSV file, Pluto automatically inspects and displays important details about",
    "details about the dataset such as its shape, column names, data types, missing values, and basic",
    "values, and basic descriptive statistics. It then analyzes the target column to determine the task",
    "determine the task type. If the target column contains categorical values, the problem is",
    "the problem is identified as a classification task. If the target column is numeric, the problem is",
    "the problem is treated as a regression task. This automatic detection ensures that users can",
    "that users can quickly begin training models without manual setup.",
    "For classification problems, Pluto supports four core algorithms: Logistic Regression, Random",
    "Regression, Random Forest Classifier, Support Vector Machine (SVM), and K-Nearest Neighbors (KNN).",
    "Neighbors (KNN). For regression problems, the available algorithms include Linear Regression,",
    "Linear Regression, Random Forest Regressor, Decision Tree Regressor, and K-Nearest Neighbors",
    "K-Nearest Neighbors Regressor. These algorithms were chosen to provide a good balance between",
    "balance between simplicity, interpretability, and predictive performance.",
    "Pluto also includes built-in visualization features that make it easy to compare model",
    "to compare model performances. It generates a bar plot showing metrics such as R\u00b2 scores or",
    "as R\u00b2 scores or accuracy for each model. The best-performing model is visually highlighted,",
    "highlighted, allowing users to quickly understand which algorithm worked best for their data. This",
    "their data. This makes the process of model evaluation both intuitive and interactive.",
    "During the training process, Pluto automatically splits the uploaded dataset into training and",
    "into training and testing subsets. It then trains all relevant models based on the detected task",
    "the detected task type and evaluates their performance using appropriate metrics. For",
    "metrics. For classification tasks, it reports metrics such as accuracy and confusion matrices,",
    "confusion matrices, while for regression tasks, it reports the R\u00b2 score and other relevant",
    "and other relevant evaluation statistics.",
    "The Pluto ML Project is built using Streamlit for the user interface, scikit-learn for machine",
    "for machine learning algorithms, pandas for data manipulation, matplotlib and seaborn for",
    "and seaborn for visualizations, and numpy for numerical operations. Together, these technologies",
    "these technologies create a seamless and efficient browser-based environment for machine learning",
    "machine learning experimentation.",
    "For example, when a user uploads the popular iris dataset, Pluto automatically recognizes it as a",
    "recognizes it as a classification problem, trains the Logistic Regression, Random Forest, SVM, and",
    "Forest, SVM, and KNN models, compares their results, and highlights the best-performing one through",
    "one through a color-coded bar chart. This fully automated workflow allows users to focus on",
    "users to focus on insights rather than technical setup.",
    "Overall, the goal of Pluto is to offer a complete no-code machine learning environment that enables",
    "that enables anyone\u2014regardless of technical expertise\u2014to explore datasets, visualize data, and",
    "visualize data, and train models directly in the browser. It emphasizes simplicity, automation, and",
    "automation, and accessibility while maintaining the flexibility and power of modern machine",
    "of modern machine learning tools."
  ]
}