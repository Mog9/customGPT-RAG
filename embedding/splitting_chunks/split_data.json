{
  "attention.txt": [
    "The attention mechanism allows a model to focus on the most relevant parts of the input when making",
    "input when making predictions. Instead of treating each word equally, attention assigns different",
    "assigns different weights to tokens based on their importance in context. For example, in the",
    "For example, in the sentence \u201cThe cat sat on the mat because it was tired,\u201d the model uses",
    "the model uses attention to understand that \u201cit\u201d refers to \u201ccat,\u201d not \u201cmat.\u201d",
    "In transformers, this is implemented as self-attention, where each token looks at every other token",
    "every other token in the sequence to decide what to focus on. This gives the model a global view of",
    "a global view of context \u2014 unlike RNNs, which could only remember recent words. Multi-head",
    "words. Multi-head attention expands this further by letting the model learn multiple types of",
    "multiple types of relationships simultaneously (e.g., syntactic and semantic).",
    "Attention is often visualized as a heatmap showing how much one word \u201cattends\u201d to another. It is",
    "to another. It is the key innovation that made transformers so effective, allowing them to handle",
    "them to handle context, ambiguity, and long sentences with incredible accuracy. In short, attention",
    "In short, attention is what gives models \u201cunderstanding\u201d in language."
  ],
  "transformers.txt": [
    "Transformers are the core architecture behind modern large language models (LLMs). Introduced by",
    "Introduced by Vaswani et al. in 2017\u2019s \u201cAttention is All You Need\u201d, the transformer revolutionized",
    "revolutionized NLP by replacing recurrent networks (RNNs) with a structure that can process entire",
    "can process entire sequences in parallel. This allowed models to scale massively and understand",
    "and understand context over long passages of text.",
    "The transformer architecture is built around two main blocks: the encoder and the decoder. The",
    "the decoder. The encoder reads and represents the input text, while the decoder generates the",
    "generates the output (for example, a translation or a chatbot response). Models like BERT use only",
    "like BERT use only the encoder for understanding tasks, while GPT uses only the decoder for",
    "the decoder for generating text. This flexibility is part of what makes transformers so powerful.",
    "Transformers also rely heavily on embeddings and attention mechanisms to understand meaning and",
    "meaning and relationships between tokens. They enable models to handle long-range dependencies \u2014",
    "dependencies \u2014 like connecting the subject and verb in a long sentence \u2014 something earlier models",
    "earlier models struggled with. Almost every modern GenAI system today, from ChatGPT to Stable",
    "ChatGPT to Stable Diffusion (for text-to-image), is built on the transformer foundation."
  ],
  "tokenization.txt": [
    "Tokenization is the process of breaking text into smaller pieces, called tokens, that can be",
    "tokens, that can be understood by language models. Tokens can be words, characters, or subwords",
    "or subwords depending on the tokenizer used. For example, \u201cunbelievable\u201d might be split into \u201cun\u201d,",
    "be split into \u201cun\u201d, \u201cbeliev\u201d, and \u201cable\u201d \u2014 this helps the model learn word patterns and meanings",
    "and meanings even for unseen words. Tokenization makes raw text mathematically representable and",
    "representable and consistent across all inputs.",
    "In practice, tokenizers use rules or pretrained vocabularies. GPT and BERT models, for instance,",
    "for instance, use Byte Pair Encoding (BPE) or WordPiece, which merge frequently occurring sequences",
    "occurring sequences of characters. This way, the tokenizer balances vocabulary size and token",
    "size and token length efficiently. These methods also handle unknown words gracefully by breaking",
    "by breaking them into known sub-pieces.",
    "Good tokenization is crucial because it directly affects how well a model can understand context.",
    "understand context. If the tokenizer splits words poorly, the model may lose semantic meaning.",
    "semantic meaning. Conversely, efficient tokenization reduces computational load and allows models",
    "and allows models to generalize across languages, typos, and new terms."
  ],
  "embedding.txt": [
    "Embeddings are vector representations of text that capture meaning in numbers. Every token (like",
    "Every token (like \u201ccat\u201d or \u201crun\u201d) is converted into a high-dimensional vector that represents its",
    "that represents its semantic context. Words that are similar in meaning have embeddings that are",
    "embeddings that are close to each other in vector space \u2014 for instance, \u201cking\u201d and \u201cqueen\u201d or",
    "and \u201cqueen\u201d or \u201cwalk\u201d and \u201crun.\u201d These embeddings become the foundation of how AI models understand",
    "models understand and compare words.",
    "Early models like Word2Vec or GloVe built embeddings by looking at how words appear in context",
    "appear in context within large text corpora. Modern transformer-based models like BERT or GPT learn",
    "BERT or GPT learn contextual embeddings, meaning the vector for a word changes depending on its",
    "depending on its surrounding words \u2014 \u201cbank\u201d in \u201criver bank\u201d and \u201cmoney bank\u201d will have different",
    "will have different vectors.",
    "Embeddings are also key in Retrieval-Augmented Generation (RAG), where your documents are converted",
    "are converted into embeddings and stored in a vector database. When a user asks a question, the",
    "a question, the system retrieves the most relevant pieces of text based on embedding similarity,",
    "similarity, enabling the model to answer accurately using external knowledge."
  ]
}